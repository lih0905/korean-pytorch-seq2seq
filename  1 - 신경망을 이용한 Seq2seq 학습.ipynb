{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ben Trevett 의 [Sequence to Sequence Learning with Neural Networks](https://github.com/bentrevett/pytorch-seq2seq/blob/master/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb) 튜토리얼을 한글 데이터셋에 적용해보는 연습이다. 데이터셋은 [AI Hub 한국어-영어 번역 말뭉치](http://www.aihub.or.kr/aidata/87/download)를 이용한다.\n",
    "\n",
    "이 모델에서는 [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)의 구조대로 간단한 모델을 만들어본다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 개요\n",
    "\n",
    "가장 유명한 seq2seq 모델은 RNN을 이용한 인코더-디코더 모델이다. 입력 문장은 인코더를 통해 하나의 문맥 벡터로 변환되고, 이후 디코더에서 타겟 문장으로 변환된다.\n",
    "\n",
    "<img src = 'https://github.com/bentrevett/pytorch-seq2seq/raw/49cbdd39d934633ab69b7ff0cf4ef0da33a42e18/assets/seq2seq1.png'>\n",
    "\n",
    "문장의 처음에 `<sos>`, 마지막에 `<eos>` 토큰을 추가하여 입력으로 넣는다. 입력 문장의 각 단어 $x_t$에 대해 벡터 임베딩을 $e(x_t)$, 이전 타임의 히든 벡터를 $h_{t-1}$ 이라 하면 인코더는 다음과 같이 정의된다.\n",
    "\n",
    "$$ h_t = \\text{EncoderRNN}(e(X_t), h_{t-1}) $$\n",
    "\n",
    "여기서 $X = \\{ x_1,\\ldots, x_T\\}$, $x_1 = <sos>$ 등이다. 마지막 단어에 대한 히든 벡터 $h_T$를 문장의 벡터로 정의한다.\n",
    "\n",
    "디코더는 타겟 문장 $(y_t)$의 임베딩 $d(y_t)$와 이전 히든 벡터 $s_{t-1}$에 대해 다음과 같이 정의된다($s_0 = h_T$).\n",
    "\n",
    "$$ s_t = \\text{DecoderRNN}(d(y_t), s_{t-1}) $$\n",
    "\n",
    "마지막으로 디코더의 출력을 선형 레이어 $f$에 태워 각 단어의 최종 스코어 $\\hat{y}_t = f(s_t)$ 를 만든다. 타겟 문장의 입력은 항상 `<sos>` 토큰으로 시작하지만, 그 이후의 벡터는 실제 타겟 단어 $y_t$를 입력을 넣을 수도 있고 아니면 이전 단계의 출력 $\\hat{y}_t$를 넣을 수도 있다. \n",
    "\n",
    "이렇게 얻은 타겟 벡터 $\\hat{Y} = \\{ \\hat{y}_1, \\ldots, \\hat{y}_T\\}$ 를 실제 정답과 비교하여 손실을 계산하고 모델을 업데이트한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전처리\n",
    "\n",
    "이후 사용할 모듈들을 불러오자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchtext.data import Field, BucketIterator, TabularDataset\n",
    "\n",
    "import spacy\n",
    "from konlpy.tag import Mecab\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "랜덤 시드를 고정하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "토크나이저를 지정하자. 여기서 한글은 [KoNLPy](https://konlpy-ko.readthedocs.io/ko/v0.4.3/)의 은전한닢, 영어는 [spaCy](https://spacy.io/)를 이용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab = Mecab() # 한글\n",
    "spacy_en = spacy.load('en') # 영어"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "토크나이저 함수를 만들자. 논문에 따르면 입력 문장은 역순으로 넣는 게 좋다고 하니 그대로 따르자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_ko(text):\n",
    "    return [tok for tok in mecab.morphs(text)][::-1]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!', '당', '어렵', '너무', '은', '한글']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_ko('한글은 너무 어렵당!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Korean', 'is', 'too', 'dificult', 'for', 'me', '!']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_en('Korean is too dificult for me!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TorchText의 `Field`를 이용해서 데이터 입력 포맷을 지정하자. `init_token`과 `eos_token` 인자를 이용해서 문장 처음과 마지막에 `<sos>`와 `<eos>` 토큰을 자동으로 추가할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = Field(tokenize = tokenize_ko,\n",
    "           init_token = '<sos>',\n",
    "           eos_token = '<eos>',\n",
    "           )\n",
    "\n",
    "TRG = Field(tokenize = tokenize_en,\n",
    "            init_token = '<sos>',\n",
    "            eos_token = '<eos>',\n",
    "            lower = True\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = {'ko': ('ko',SRC), 'en': ('en',TRG)}\n",
    "# dictionary 형식은 {csv컬럼명 : (데이터 컬럼명, Field이름)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.구어체.xlsx  3.문어체-뉴스.xlsx  train_data.csv\n",
      "2.대화체.xlsx  test_data.csv\t   valid_data.csv\n"
     ]
    }
   ],
   "source": [
    "!ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = TabularDataset.splits(\n",
    "                            path = 'data',\n",
    "                            train = 'train_data.csv',\n",
    "                            test = 'test_data.csv',\n",
    "                            format = 'csv',\n",
    "                            fields = fields,  \n",
    ")\n",
    "valid_data = TabularDataset(path = 'data/valid_data.csv',\n",
    "                            format = 'csv',\n",
    "                            fields = fields,  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "불러온 데이터는 다음과 같은 형태이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'ko': ['.', '요', '가', '안', '가', '이해', '이', '문장', '이', '님', '선생'],\n",
       "  'en': ['sir',\n",
       "   ',',\n",
       "   'i',\n",
       "   'do',\n",
       "   \"n't\",\n",
       "   'understand',\n",
       "   'this',\n",
       "   'sentence',\n",
       "   'here',\n",
       "   '.']},\n",
       " {'ko': ['.', '가요', '로', '기숙사', '자마자', '끝나', '가', '학교'],\n",
       "  'en': ['i',\n",
       "   'go',\n",
       "   'to',\n",
       "   'dormitory',\n",
       "   'as',\n",
       "   'soon',\n",
       "   'as',\n",
       "   'i',\n",
       "   'finished',\n",
       "   'class',\n",
       "   '.']})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(train_data[0]), vars(valid_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "길이를 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 94463\n",
      "Number of validation examples: 31688\n",
      "Number of testing examples: 31822\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
    "print(f\"Number of testing examples: {len(test_data.examples)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 단어장을 만들자. `min_freq` 옵션을 이용하여 최소 2번 이상 등장하는 단어만 사용하도록 하자. 또한 단어장은 검증/테스트셋은 써서는 안된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.build_vocab(train_data, min_freq=2)\n",
    "TRG.build_vocab(train_data, min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in source (ko) vocabulary: 33345\n",
      "Unique tokens in target (en) vocabulary: 24619\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in source (ko) vocabulary: {len(SRC.vocab)}\")\n",
    "print(f\"Unique tokens in target (en) vocabulary: {len(TRG.vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이터레이터를 만들자. 일반적인 `Iterator` 대신 `BucketIterator` 쓰면 입력/출력 배치 안에 있는 문장의 길이가 최대한 비슷하게 되어 패딩을 최소화하게 해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2seq 모델 생성\n",
    "\n",
    "인코더, 디코더, 그리고 seq2seq 모델 순서대로 만들자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 인코더\n",
    "\n",
    "인코더는 2레이어 LSTM을 사용하여 다음과 같이 정의한다.\n",
    "\n",
    "$$ (h_t^1,c_t^1)= \\text{EncoderLSTM}^1(e(x_t),(h_{t-1}^1,c_{t-1}^1)) $$\n",
    "\n",
    "$$ (h_t^2,c_t^2)= \\text{EncoderLSTM}^2(h_t^1,(h_{t-1}^2, c_{t-1}^2)) $$\n",
    "\n",
    "<img src = 'https://github.com/bentrevett/pytorch-seq2seq/raw/49cbdd39d934633ab69b7ff0cf4ef0da33a42e18/assets/seq2seq2.png'>\n",
    "\n",
    "인코더는 다음 인수들을 입력으로 받는다.\n",
    "\n",
    "* `input_dim` : 입력 문장의 단어 갯수\n",
    "* `emb_dim` : 임베딩 차원\n",
    "* `hid_dim` : 히든 차원\n",
    "* `n_layers` : RNN 계층수\n",
    "* `dropout` : dropout 비율. 계층 사이에 적용된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
